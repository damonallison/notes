{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "From [Quick Start](https://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "\t3.7.0 (default, Jul 10 2018, 07:48:31) \n",
      "[Clang 9.1.0 (clang-902.0.39.2)]\n",
      "\n",
      "\n",
      "\n",
      "Environment:\n",
      "\tenviron({'SPARK_HOME': '/usr/local/Cellar/apache-spark/2.3.2/libexec', 'TERM_PROGRAM': 'iTerm.app', 'rvm_bin_path': '/Users/dra/.rvm/bin', 'GEM_HOME': '/Users/dra/.rvm/gems/ruby-2.5.1@ios', 'NVM_CD_FLAGS': '', 'SHELL': '/usr/local/bin/fish', 'TERM': 'xterm-color', 'OMF_CONFIG': '/Users/dra/.config/omf', 'TMPDIR': '/var/folders/60/9grv09hj6c39tv4rlmyjc4pm94m_nc/T/', 'Apple_PubSub_Socket_Render': '/private/tmp/com.apple.launchd.uy4QaOFFql/Render', 'TERM_PROGRAM_VERSION': '3.2.5', 'TERM_SESSION_ID': 'w0t0p0:7C79ECDD-608F-4371-85BB-54AD8841C156', 'SPARK_CONF_DIR': '/usr/local/Cellar/apache-spark/2.3.2/libexec/conf', 'NVM_DIR': '/Users/dra/.nvm', 'USER': 'dra', 'COMMAND_MODE': 'unix2003', 'PYSPARK_PYTHON': 'python', 'rvm_path': '/Users/dra/.rvm', 'PYSPARK_DRIVER_PYTHON': 'jupyter', 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.nLcGXejecb/Listeners', '__CF_USER_TEXT_ENCODING': '0x124A2AAC:0x0:0x0', 'rvm_prefix': '/Users/dra', 'PATH': '/usr/local/bin:/Users/dra/.rvm/gems/ruby-2.5.1@ios/bin:/Users/dra/.rvm/gems/ruby-2.5.1@global/bin:/Users/dra/.rvm/rubies/ruby-2.5.1/bin:/Users/dra/.rvm/bin:/bin:/Users/dra/.nvm/versions/node/v9.11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet', 'PYTHONHASHSEED': '0', 'PWD': '/Users/dra/projects/notes/data-science/spark', 'JAVA_HOME': '/Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home', 'LANG': 'en_US.UTF-8', 'PYTHONSTARTUP': '/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/shell.py', 'ITERM_PROFILE': 'Default', 'XPC_FLAGS': '0x0', 'SPARK_ENV_LOADED': '1', 'XPC_SERVICE_NAME': '0', 'rvm_version': '1.29.4 master', 'SPARK_LOCAL_IP': '127.0.0.1', 'COLORFGBG': '7;0', 'HOME': '/Users/dra', 'SHLVL': '1', 'OMF_PATH': '/Users/dra/.local/share/omf', 'rvm_ruby_string': 'ruby-2.5.1', 'ITERM_SESSION_ID': 'w0t0p0:7C79ECDD-608F-4371-85BB-54AD8841C156', 'OLD_PYTHONSTARTUP': '', 'PYTHONPATH': '/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip:/usr/local/Cellar/apache-spark/2.3.2/libexec/python/:', '_SPARK_CMD_USAGE': 'Usage: ./bin/pyspark [options]', 'LOGNAME': 'dra', 'GEM_PATH': '/Users/dra/.rvm/gems/ruby-2.5.1@ios:/Users/dra/.rvm/gems/ruby-2.5.1@global', 'NVM_BIN': '/Users/dra/.nvm/versions/node/v9.11.2/bin', 'PYSPARK_DRIVER_PYTHON_OPTS': 'notebook', 'rvm_delete_flag': '0', 'SPARK_SCALA_VERSION': '2.12', 'SECURITYSESSIONID': '186aa', 'COLORTERM': 'truecolor', 'PYSPARK_SUBMIT_ARGS': '\"--name\" \"PySparkShell\" \"pyspark-shell\"', '__PYVENV_LAUNCHER__': '/usr/local/Cellar/python/3.7.0/bin/python3.7', 'JPY_PARENT_PID': '7494', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'})\n"
     ]
    }
   ],
   "source": [
    "# Print environment information\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python version:\\n\\t{sys.version}\")\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Environment:\\n\\t{repr(os.environ)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1116d3710>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the spark context.\n",
    "#\n",
    "# pyspark will create a SparkContext. You can configure the SparkContext by passing command line args to `pyspark`\n",
    "#\n",
    "# Adds test.py to the path - which can be imported later.\n",
    "#\n",
    "# $ pyspark --py-files test.py\n",
    "#\n",
    "# You can customize the Jupyter commands using the PYSPARK_DRIVER_PYTHON_OPTS environment variables.\n",
    "# \n",
    "# These variables are set at: ~/.config/fish/config.fish\n",
    "# set --export PYSPARK_DRIVER_PYTHON jupyter\n",
    "# set --export PYSPARK_DRIVER_PYTHON_OPTS 'notebook'\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# DataFrame\n",
    "#\n",
    "# A DataFrame is a Dataset (Scala, Java API) organized into named columns.\n",
    "# it is conceptually equivalent to a table in a relational database or a data frame in R / Python.\n",
    "# \n",
    "# Spark supports creating DataFrame(s) using multiple input sources:\n",
    "# \n",
    "# * Text files (text, json, parquet, csv)\n",
    "# * JDBC\n",
    "# * Hive\n",
    "#\n",
    "#\n",
    "# Creates a DataFrame by reading in a textfile of lines.\n",
    "# \n",
    "# NOTE: For local files, the file must also be accessible at the same path on worker nodes. Either\n",
    "#       use HDFS or a network file share when operating in a cluster.\n",
    "#\n",
    "namesDF = spark.read.text(\"data/names.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "count == 100000\n",
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|   Sharon Payne|\n",
      "| Clayton Harper|\n",
      "| Joanne Johnson|\n",
      "|   Kelly Wilson|\n",
      "| Taylor Sanford|\n",
      "|   Sarah Miller|\n",
      "|     Sean Barry|\n",
      "|     Adam Smith|\n",
      "|   George Mccoy|\n",
      "|Jeffrey Gregory|\n",
      "|   Charles Hall|\n",
      "|  Pamela Wright|\n",
      "|    Bryan Scott|\n",
      "|  William Wells|\n",
      "|    Stacy Moore|\n",
      "|  Jeremy Barber|\n",
      "| Joshua Murillo|\n",
      "|Dr. Kendra Hall|\n",
      "|  Angela Warren|\n",
      "|  Marcus Brooks|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='Sharon Payne'),\n",
       " Row(value='Clayton Harper'),\n",
       " Row(value='Joanne Johnson'),\n",
       " Row(value='Kelly Wilson'),\n",
       " Row(value='Taylor Sanford'),\n",
       " Row(value='Sarah Miller'),\n",
       " Row(value='Sean Barry'),\n",
       " Row(value='Adam Smith'),\n",
       " Row(value='George Mccoy'),\n",
       " Row(value='Jeffrey Gregory')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Exploring a DataFrame\n",
    "#\n",
    "\n",
    "namesDF.printSchema()\n",
    "\n",
    "print(f\"count == {namesDF.count()}\")\n",
    "\n",
    "# Examining data\n",
    "namesDF.select(\"value\").show()\n",
    "namesDF.select(\"value\").take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike count is: 68\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# DataFrames support two types of operations:\n",
    "#\n",
    "# * Transformations (map, filter) - return new DataFrames.\n",
    "# * Actions (reduce) - collects results.\n",
    "#\n",
    "# Transformations are lazy - they are not executed until an action is performed.\n",
    "#\n",
    "# Create a new Dataset by filtering the existing one.\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# \n",
    "# Create a new DataFrame by filtering the original. Note this DF is not \n",
    "# materialized until the .count() action occurs.\n",
    "# \n",
    "# Regex filter for `mike`, effectively case insensitive.\n",
    "#\n",
    "\n",
    "mikeDF = namesDF.filter(lower(namesDF[\"value\"]).rlike(\"mike\"))\n",
    "print(f\"Mike count is: {mikeDF.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the line with the most words\n",
    "namesDF.select(size(split(namesDF.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='Michael', count=2270),\n",
       " Row(word='Smith', count=2133),\n",
       " Row(word='James', count=1705),\n",
       " Row(word='Johnson', count=1639),\n",
       " Row(word='David', count=1584),\n",
       " Row(word='John', count=1443),\n",
       " Row(word='Jennifer', count=1428),\n",
       " Row(word='Thomas', count=1422),\n",
       " Row(word='Williams', count=1403),\n",
       " Row(word='Christopher', count=1395)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MapReduce Example\n",
    "# \n",
    "# Use the explode function in select to transform a Dataset of lines into a Dataset of words, and then combine\n",
    "# them with groupBy and count to compute the per-word countes in the file as a Dataset of 2 columns - \"word\" and \"count\"\n",
    "\n",
    "wordCountsDF = namesDF.select(explode(split(namesDF.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "wordCountsDF.printSchema()\n",
    "wordCountsDF.sort(col(\"count\"), ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordCountsDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bd9fb1d8c86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This is useful when data is accessed multiple times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwordCountsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wordCountsDf' is not defined"
     ]
    }
   ],
   "source": [
    "# Cache the dataset into a cluster-wide in-memory cache.\n",
    "# \n",
    "# This is useful when data is accessed multiple times.\n",
    "\n",
    "wordCountsDf.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          value|\n",
      "+---------------+\n",
      "|   Sharon Payne|\n",
      "| Clayton Harper|\n",
      "| Joanne Johnson|\n",
      "|   Kelly Wilson|\n",
      "| Taylor Sanford|\n",
      "|   Sarah Miller|\n",
      "|     Sean Barry|\n",
      "|     Adam Smith|\n",
      "|   George Mccoy|\n",
      "|Jeffrey Gregory|\n",
      "|   Charles Hall|\n",
      "|  Pamela Wright|\n",
      "|    Bryan Scott|\n",
      "|  William Wells|\n",
      "|    Stacy Moore|\n",
      "|  Jeremy Barber|\n",
      "| Joshua Murillo|\n",
      "|Dr. Kendra Hall|\n",
      "|  Angela Warren|\n",
      "|  Marcus Brooks|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Perform SQL operations on a DataFrame, returning results as a new DataFrame.\n",
    "# \n",
    "# Note: SQL operations can only be executed on DataFrames which have been registered\n",
    "# as a table.\n",
    "# \n",
    "\n",
    "# Register the DataFrame as a temporary view.\n",
    "#\n",
    "# Temporary views are session-scoped. To have a temporary view that is shared among\n",
    "# all sessions and kept alive until Spark terminates, use `createOrReplaceGlobalTempView`.\n",
    "# \n",
    "namesDF.createOrReplaceTempView(\"names\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM names\")\n",
    "\n",
    "sqlDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
