{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "From [Quick Start](https://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "\t3.7.0 (default, Jul 10 2018, 07:48:31) \n",
      "[Clang 9.1.0 (clang-902.0.39.2)]\n",
      "\n",
      "\n",
      "\n",
      "Environment:\n",
      "\tenviron({'SPARK_HOME': '/usr/local/Cellar/apache-spark/2.3.2/libexec', 'TERM_PROGRAM': 'iTerm.app', 'rvm_bin_path': '/Users/dra/.rvm/bin', 'GEM_HOME': '/Users/dra/.rvm/gems/ruby-2.5.1@ios', 'NVM_CD_FLAGS': '', 'SHELL': '/usr/local/bin/fish', 'TERM': 'xterm-color', 'OMF_CONFIG': '/Users/dra/.config/omf', 'TMPDIR': '/var/folders/60/9grv09hj6c39tv4rlmyjc4pm94m_nc/T/', 'Apple_PubSub_Socket_Render': '/private/tmp/com.apple.launchd.uy4QaOFFql/Render', 'TERM_PROGRAM_VERSION': '3.2.5', 'TERM_SESSION_ID': 'w0t0p0:7C79ECDD-608F-4371-85BB-54AD8841C156', 'SPARK_CONF_DIR': '/usr/local/Cellar/apache-spark/2.3.2/libexec/conf', 'NVM_DIR': '/Users/dra/.nvm', 'USER': 'dra', 'COMMAND_MODE': 'unix2003', 'PYSPARK_PYTHON': 'python', 'rvm_path': '/Users/dra/.rvm', 'PYSPARK_DRIVER_PYTHON': 'jupyter', 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.nLcGXejecb/Listeners', '__CF_USER_TEXT_ENCODING': '0x124A2AAC:0x0:0x0', 'rvm_prefix': '/Users/dra', 'PATH': '/usr/local/bin:/Users/dra/.rvm/gems/ruby-2.5.1@ios/bin:/Users/dra/.rvm/gems/ruby-2.5.1@global/bin:/Users/dra/.rvm/rubies/ruby-2.5.1/bin:/Users/dra/.rvm/bin:/bin:/Users/dra/.nvm/versions/node/v9.11.2/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet', 'PYTHONHASHSEED': '0', 'PWD': '/Users/dra/projects/notes', 'JAVA_HOME': '/Library/Java/JavaVirtualMachines/jdk1.8.0_73.jdk/Contents/Home', 'LANG': 'en_US.UTF-8', 'PYTHONSTARTUP': '/usr/local/Cellar/apache-spark/2.3.2/libexec/python/pyspark/shell.py', 'ITERM_PROFILE': 'Default', 'XPC_FLAGS': '0x0', 'SPARK_ENV_LOADED': '1', 'XPC_SERVICE_NAME': '0', 'rvm_version': '1.29.4 master', 'SPARK_LOCAL_IP': '127.0.0.1', 'COLORFGBG': '7;0', 'HOME': '/Users/dra', 'SHLVL': '1', 'OMF_PATH': '/Users/dra/.local/share/omf', 'rvm_ruby_string': 'ruby-2.5.1', 'ITERM_SESSION_ID': 'w0t0p0:7C79ECDD-608F-4371-85BB-54AD8841C156', 'OLD_PYTHONSTARTUP': '', 'PYTHONPATH': '/usr/local/Cellar/apache-spark/2.3.2/libexec/python/lib/py4j-0.10.7-src.zip:/usr/local/Cellar/apache-spark/2.3.2/libexec/python/:', '_SPARK_CMD_USAGE': 'Usage: ./bin/pyspark [options]', 'LOGNAME': 'dra', 'GEM_PATH': '/Users/dra/.rvm/gems/ruby-2.5.1@ios:/Users/dra/.rvm/gems/ruby-2.5.1@global', 'NVM_BIN': '/Users/dra/.nvm/versions/node/v9.11.2/bin', 'PYSPARK_DRIVER_PYTHON_OPTS': 'notebook', 'rvm_delete_flag': '0', 'SPARK_SCALA_VERSION': '2.12', 'SECURITYSESSIONID': '186aa', 'COLORTERM': 'truecolor', 'PYSPARK_SUBMIT_ARGS': '\"--name\" \"PySparkShell\" \"pyspark-shell\"', '__PYVENV_LAUNCHER__': '/usr/local/Cellar/python/3.7.0/bin/python3.7', 'JPY_PARENT_PID': '54891', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'})\n"
     ]
    }
   ],
   "source": [
    "# Print environment information\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python version:\\n\\t{sys.version}\")\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Environment:\\n\\t{repr(os.environ)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109e9a3c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the spark context.\n",
    "#\n",
    "# pyspark will create a SparkContext. You can configure the SparkContext by passing command line args to `pyspark`\n",
    "#\n",
    "# Adds test.py to the path - which can be imported later.\n",
    "#\n",
    "# $ pyspark --py-files test.py\n",
    "#\n",
    "# You can customize the Jupyter commands using the PYSPARK_DRIVER_PYTHON_OPTS environment variables.\n",
    "# \n",
    "# These variables are set at: ~/.config/fish/config.fish\n",
    "# set --export PYSPARK_DRIVER_PYTHON jupyter\n",
    "# set --export PYSPARK_DRIVER_PYTHON_OPTS 'notebook'\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count is: 5\n"
     ]
    }
   ],
   "source": [
    "# Read a text file into an RDD of a collection of lines.\n",
    "#\n",
    "# NOTE: For local files, the file must also be accessible at the same path on worker nodes. Either\n",
    "#       use HDFS or a network file share when operating in a cluster.\n",
    "\n",
    "textFile = spark.read.text(\"data/names.txt\")\n",
    "print(f\"Count is: {textFile.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damon count is: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109e9a3c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# RDDs support two types of operations:\n",
    "#\n",
    "# * Transformations (map, filter) - return new RDDs.\n",
    "# * Actions (reduce) - collects results.\n",
    "#\n",
    "# Transformations are lazy - they are not executed until an action is performed.\n",
    "#\n",
    "# Create a new Dataset by filtering the existing one.\n",
    "#\n",
    "\n",
    "linesWithDamon = textFile.filter(textFile.value.contains(\"damon\"))\n",
    "print(f\"Damon count is: {linesWithDamon.count()}\")\n",
    "\n",
    "spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the line with the most words\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MapReduce Example\n",
    "# \n",
    "# Use the explode function in select to transform a Dataset of lines into a Dataset of words, and then combine\n",
    "# them with groupBy and count to compute the per-word countes in the file as a Dataset of 2 columns - \"word\" and \"count\"\n",
    "\n",
    "wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='elizabeth', count=1),\n",
       " Row(word='kari', count=1),\n",
       " Row(word='damon', count=1),\n",
       " Row(word='grace', count=1),\n",
       " Row(word='allison', count=4),\n",
       " Row(word='lily', count=1),\n",
       " Row(word='cole', count=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cache the dataset into a cluster-wide in-memory cache.\n",
    "# \n",
    "# This is useful when data is accessed multiple times.\n",
    "\n",
    "wordCounts.cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
